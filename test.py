from bs4 import BeautifulSoup  # ç½‘é¡µè§£æï¼Œè·å–æ•°æ®
import re  # æ­£åˆ™è¡¨è¾¾å¼ï¼Œè¿›è¡Œæ–‡å­—åŒ¹é…`
import urllib.request, urllib.error  # åˆ¶å®šURLï¼Œè·å–ç½‘é¡µæ•°æ®
import requests

# ç”Ÿæˆæ–‡ä»¶å
def generate_filename(url):
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œå°†URLä¸­çš„ç‰¹æ®Šå­—ç¬¦è½¬æ¢ä¸ºä¸‹åˆ’çº¿"""
    # æ›¿æ¢åè®®ä¸­çš„://
    filename = re.sub(r'://', '_', url)
    # ç§»é™¤éå­—æ¯æ•°å­—ã€ç‚¹ã€è¿å­—ç¬¦ã€ä¸‹åˆ’çº¿çš„å­—ç¬¦
    filename = re.sub(r'[^\w\.-]', '_', filename)
    # åˆå¹¶å¤šä¸ªè¿ç»­çš„ä¸‹åˆ’çº¿
    filename = re.sub(r'_+', '_', filename)
    # å»é™¤é¦–å°¾çš„ä¸‹åˆ’çº¿å¹¶æ·»åŠ .txtæ‰©å±•å
    filename = filename.strip('_')
    if not filename:
        filename = 'default'
    return f"{filename}.txt"

# è§£ç æ–‡ä»¶
def decode_content(content, response):
    """è‡ªåŠ¨æ£€æµ‹å†…å®¹ç¼–ç å¹¶è§£ç """
    # ä»HTTPå¤´è·å–ç¼–ç 
    charset = response.headers.get_content_charset()

    # ä»HTML metaæ ‡ç­¾æ£€æµ‹ç¼–ç 
    if not charset:
        soup = BeautifulSoup(content, 'html.parser')
        meta_charset = soup.find('meta', {'charset': True})
        if meta_charset:
            charset = meta_charset['charset']
        else:
            meta_http_equiv = soup.find('meta', attrs={'http-equiv': re.compile(r'content-type', re.I)})
            if meta_http_equiv and 'content' in meta_http_equiv.attrs:
                match = re.search(r'charset=([\w-]+)', meta_http_equiv['content'], re.I)
                if match:
                    charset = match.group(1)

    # å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è§£ç 
    if charset:
        try:
            return content.decode(charset)
        except (UnicodeDecodeError, LookupError):
            pass

    # å¸¸è§ç¼–ç å…œåº•
    encodings = ['utf-8', 'gbk', 'latin-1', 'iso-8859-1']
    for encoding in encodings:
        try:
            return content.decode(encoding)
        except UnicodeDecodeError:
            continue

    # æœ€ç»ˆå°è¯•ç”¨æ›¿æ¢é”™è¯¯å¤„ç†
    return content.decode('utf-8', errors='replace')

# è·å–ç½‘é¡µæ•°æ®
def fetch_and_save(url):
    """è·å–å¹¶ä¿å­˜ç½‘é¡µå†…å®¹"""
    try:
        response = urllib.request.urlopen(url)
        content = response.read()
    except urllib.error.URLError as e:
        print(f"âš ï¸ è¿æ¥å¤±è´¥: {e.reason}")
        return
    except urllib.error.HTTPError as e:
        print(f"â›” HTTPé”™è¯¯ {e.code}: {e.reason}")
        return
    except Exception as e:
        print(f"âŒ å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}")
        return

    # è§£ç å†…å®¹
    decoded_content = decode_content(content, response)

    # ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜
    filename = generate_filename(url)
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(decoded_content)
        print(f"âœ… ç½‘é¡µå·²ä¿å­˜ä¸º [{filename}]")
    except IOError as e:
        print(f"ğŸ–¥ï¸ æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    input_url = input("ğŸŒ è¯·è¾“å…¥è¦çˆ¬å–çš„URL: ")
    fetch_and_save(input_url)
