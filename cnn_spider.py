# cnn_spider.py
import json
from lxml import etree
from utils import BaseSpider, Saver, logger


class CNNSpider(BaseSpider):
    """CNNæ–°é—»çˆ¬è™«"""

    def __init__(self, config=None):
        super().__init__("cnn", config)
        self.base_urls = [
            "https://edition.cnn.com/world",
            "https://edition.cnn.com/politics",
            "https://edition.cnn.com/business",
            "https://edition.cnn.com/tech",
            "https://edition.cnn.com/health",
            "https://edition.cnn.com/entertainment",
            "https://edition.cnn.com/us",
            "https://edition.cnn.com/travel",
            "https://edition.cnn.com/style",
        ]

    def crawl(self, max_articles=50):
        """ä¸»çˆ¬å–æµç¨‹"""
        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ–‡ç« é“¾æ¥
        article_urls = self.parallel_execute(self.base_urls, self._crawl_section)
        article_urls = list(set(article_urls))[:max_articles]
        logger.info(f"ğŸ”— æ€»å…± {len(article_urls)} ç¯‡æ–°é—»")

        # ç¬¬äºŒé˜¶æ®µï¼šçˆ¬å–æ–‡ç« å†…å®¹
        articles = self.parallel_execute(article_urls, self._crawl_article)

        # ä¿å­˜ç»“æœ
        if articles:
            self._save_results(articles)
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(articles)} ç¯‡CNNæ–°é—»")
        else:
            logger.info("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆæ–°é—»")

    def _crawl_section(self, url):
        """çˆ¬å–æ¿å—é¡µé¢"""
        logger.info(f"â³ å¼€å§‹çˆ¬å–æ¿å—: {url}")
        content = self.fetcher.fetch_and_save(url, language="CNN")
        # æå–æ–‡ç« é“¾æ¥
        if not content:
            return []
        # å…ˆæå–é“¾æ¥å†è®°å½•æ•°é‡
        links = self._extract_links(content)
        logger.info(f"ğŸ”— å‘ç° {len(links)} ç¯‡æ–°é—»")
        return self._extract_links(content) if content else []

    # def _extract_links(self, content):
    #     """ä»HTMLä¸­æå–æ–‡ç« é“¾æ¥"""
    #     html = etree.HTML(content)
    #     return [
    #         f"https://edition.cnn.com{link}"
    #         for link in html.xpath('//a[contains(@href, "/202")]/@href')
    #         if link.startswith('/202')
    #     ]
    def _extract_links(self, content):
        """ä»HTMLä¸­æå–æ–‡ç« é“¾æ¥ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        try:
            html = etree.HTML(content)
            # æ›´ç²¾å‡†çš„XPathåŒ¹é…ï¼Œç¤ºä¾‹ï¼šåŒ¹é…åŒ…å«æ—¥æœŸæ ¼å¼çš„è·¯å¾„
            raw_links = html.xpath('//a[contains(@href, "/20")]/@href')  # è°ƒæ•´XPathé€»è¾‘
            # è¿‡æ»¤æœ‰æ•ˆé“¾æ¥å¹¶å»é‡
            valid_links = {
                link for link in raw_links
                if link.startswith('/202') and len(link.split('/')) > 3  # ç®€å•æ ¡éªŒè·¯å¾„æ·±åº¦
            }
            return [f"https://edition.cnn.com{link}" for link in valid_links]
        except Exception as e:
            logger.error(f"è§£æé“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _crawl_article(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        self.random_delay()
        logger.info(f"â³ å¼€å§‹çˆ¬å–æ–‡ç« : {url}")

        content = self.fetcher.fetch_and_save(url, language="CNN", save_origin=True)
        return self._parse_article(content, url) if content else None

    def _parse_article(self, content, url):
        """è§£ææ–‡ç« å†…å®¹"""
        html = etree.HTML(content)
        article = {
            "title": "No Title",
            "url": url,
            "author": "Unknown",
            "publish_time": "",
            "content": ""
        }

        # ä»JSON-LDæå–ç»“æ„åŒ–æ•°æ®
        ld_json = self._extract_json_ld(html)
        if ld_json:
            article.update({
                "title": ld_json.get("headline", "").strip(),
                "author": self._parse_authors(ld_json.get("author", [])),
                "publish_time": ld_json.get("datePublished", "").strip()
            })

        # HTMLé™çº§è§£æ
        article["title"] = html.xpath('//h1/text()')[0].strip() if not article["title"] else article["title"]
        article["content"] = "\n\n".join([
            ' '.join(p.xpath('.//text()')).strip()
            for p in html.xpath('//div[contains(@class, "article__content")]//p')
        ])
        return article

    def _extract_json_ld(self, html):
        """æå–ç»“æ„åŒ–æ•°æ®"""
        for script in html.xpath('//script[@type="application/ld+json"]/text()'):
            try:
                data = json.loads(script.strip())
                if isinstance(data, list):
                    data = next((item for item in data if item.get("@type") == "NewsArticle"), None)
                if data and data.get("@type") == "NewsArticle":
                    return data
            except json.JSONDecodeError:
                continue
        return None

    def _parse_authors(self, authors):
        """è§£æä½œè€…ä¿¡æ¯"""
        if isinstance(authors, dict):
            authors = [authors]
        return ", ".join([
            auth.get("name", "").strip()
            for auth in authors
            if isinstance(auth, dict)
        ]) or "Unknown"

    def _save_results(self, data):
        """ä¿å­˜è§£æç»“æœ"""
        Saver.save_data(
            data,
            save_dir=f"parsed/{self.name}",
            # exclude_columns=['raw_content'],
            exclude_columns="content",
            format_type='both'
        )
