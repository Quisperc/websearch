# cnn_spider.py ï¼ˆåˆ†ç±»è¿‡äºéº»çƒ¦ï¼Œå·²å¼ƒç”¨ï¼‰
import json

from lxml import etree

from utils.BaseSpider import BaseSpider
from utils.Saver import Saver
from utils.TqdmLogHandler import logger


class CNNSpider(BaseSpider):
    """CNNæ–°é—»çˆ¬è™«

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - å¤šé¢‘é“å¹¶è¡Œçˆ¬å–ï¼ˆå›½é™…/æ”¿æ²»/å•†ä¸š/ç§‘æŠ€ç­‰9ä¸ªæ¿å—ï¼‰
    - åŒé˜¶æ®µé‡‡é›†æµç¨‹ï¼ˆé“¾æ¥æ”¶é›† + å†…å®¹æŠ“å–ï¼‰
    - ç»“æ„åŒ–æ•°æ®ä¼˜å…ˆè§£æï¼ˆJSON-LDï¼‰
    - HTMLé™çº§è§£æç­–ç•¥
    - æ™ºèƒ½å»é‡ä¸ç»“æœæŒä¹…åŒ–

    è®¾è®¡ç‰¹ç‚¹ï¼š
    - ç»§æ‰¿BaseSpideråŸºç±»å®ç°å®šåˆ¶é€»è¾‘
    - æ”¯æŒéšæœºå»¶è¿Ÿé˜²å°ç¦
    - è‡ªåŠ¨ç¼–ç å¤„ç†ä¸å†…å®¹ç¼“å­˜
    - è¿›åº¦å¯è§†åŒ–ä¸å¼‚å¸¸æ•è·
    """

    def __init__(self, config=None):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹

        Args:
            config (dict, optional): çˆ¬è™«é…ç½®å‚æ•°ï¼Œå¯è¦†ç›–é»˜è®¤è®¾ç½®
        """
        super().__init__("cnn", config)  # ç»§æ‰¿åŸºç±»åˆå§‹åŒ–é€»è¾‘
        # CNNå„é¢‘é“å…¥å£URLåˆ—è¡¨
        self.base_urls = [
            "https://edition.cnn.com/world",  # å›½é™…æ–°é—»
            "https://edition.cnn.com/politics",  # æ”¿æ²»æ–°é—»
            "https://edition.cnn.com/business",  # å•†ä¸šæ–°é—»
            "https://edition.cnn.com/tech",  # ç§‘æŠ€æ–°é—»
            "https://edition.cnn.com/health",  # å¥åº·æ–°é—»
            "https://edition.cnn.com/entertainment",  # å¨±ä¹æ–°é—»
            "https://edition.cnn.com/us",  # ç¾å›½æ–°é—»
            "https://edition.cnn.com/travel",  # æ—…æ¸¸æ–°é—»
            "https://edition.cnn.com/style",  # æ—¶å°šæ–°é—»
        ]

    def crawl(self, max_articles=50):
        """ä¸»çˆ¬å–æ§åˆ¶æµç¨‹

        æ‰§è¡Œæ­¥éª¤ï¼š
        1. å¹¶è¡Œçˆ¬å–å„æ¿å—è·å–æ–‡ç« é“¾æ¥
        2. é“¾æ¥å»é‡ä¸æ•°é‡æ§åˆ¶
        3. å¹¶è¡Œçˆ¬å–æ–‡ç« è¯¦æƒ…å†…å®¹
        4. ç»“æœæŒä¹…åŒ–å­˜å‚¨

        Args:
            max_articles (int): æœ€å¤§æŠ“å–æ•°é‡ï¼Œé»˜è®¤50ç¯‡
        """
        # ç¬¬ä¸€é˜¶æ®µï¼šåˆ†å¸ƒå¼æ”¶é›†æ–‡ç« é“¾æ¥
        article_urls = self.parallel_execute(self.base_urls, self._crawl_section)
        article_urls = list(set(article_urls))[:max_articles]  # å»é‡+æ•°é‡æ§åˆ¶
        logger.info(f"ğŸ”— æ€»å…± {len(article_urls)} ç¯‡æ–°é—»")

        # ç¬¬äºŒé˜¶æ®µï¼šå¹¶è¡Œè·å–æ–‡ç« å†…å®¹
        articles = self.parallel_execute(article_urls, self._crawl_article)

        # æ•°æ®æŒä¹…åŒ–
        if articles:
            self._save_results(articles)
            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(articles)} ç¯‡CNNæ–°é—»")
        else:
            logger.info("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆæ–°é—»")

    def _crawl_section(self, url):
        """çˆ¬å–æ–°é—»æ¿å—é¡µé¢

        æ‰§è¡Œæ­¥éª¤ï¼š
        - è·å–å¹¶ç¼“å­˜é¡µé¢å†…å®¹
        - æå–æœ‰æ•ˆæ–‡ç« é“¾æ¥
        - è®°å½•é‡‡é›†çŠ¶æ€

        Args:
            url (str): æ¿å—å…¥å£URL

        Returns:
            list: æœ¬æ¿å—æå–åˆ°çš„æ–‡ç« é“¾æ¥åˆ—è¡¨
        """
        logger.info(f"â³ å¼€å§‹çˆ¬å–æ¿å—: {url}")
        content = self.fetcher.fetch_and_save(url, direction="CNN")  # å¸¦è‡ªåŠ¨ç¼“å­˜çš„è¯·æ±‚

        # å†…å®¹æœ‰æ•ˆæ€§æ ¡éªŒ
        if not content:
            return []

        # æå–å¹¶è¿”å›æ–‡ç« é“¾æ¥ï¼ˆå¸¦æ—¥å¿—è®°å½•ï¼‰
        links = self._extract_links(content)
        logger.info(f"ğŸ”— å‘ç° {len(links)} ç¯‡æ–°é—»")
        return links

    def _extract_links(self, content):
        """ä»HTMLä¸­ç²¾å‡†æå–æ–‡ç« é“¾æ¥

        æ”¹è¿›ç‚¹ï¼š
        - å¢å¼ºXPathé€‰æ‹©å™¨å‡†ç¡®æ€§
        - æ·»åŠ è·¯å¾„æ·±åº¦æ ¡éªŒ
        - åŠ å…¥å¼‚å¸¸å¤„ç†æœºåˆ¶

        Args:
            content (str): HTMLå†…å®¹

        Returns:
            list: æ ‡å‡†åŒ–åçš„å®Œæ•´æ–‡ç« URLåˆ—è¡¨
        """
        try:
            html = etree.HTML(content)
            # å®½æ¾åŒ¹é…å¯èƒ½åŒ…å«æ–‡ç« çš„é“¾æ¥
            raw_links = html.xpath('//a[contains(@href, "/20")]/@href')  # åŒ…å«å¹´ä»½çš„è·¯å¾„

            # è¿‡æ»¤è§„åˆ™ï¼š
            # 1. è·¯å¾„ä»¥/202å¼€å¤´ï¼ˆCNNæ–‡ç« è·¯å¾„ç‰¹å¾ï¼‰
            # 2. è·¯å¾„æ®µæ•°å¤§äº3ï¼ˆæ’é™¤ç®€å•é¡µé¢ï¼‰
            valid_links = {
                link for link in raw_links
                if link.startswith('/202') and len(link.split('/')) > 3
            }
            # ç”Ÿæˆå®Œæ•´URLå¹¶è¿”å›
            return [f"https://edition.cnn.com{link}" for link in valid_links]
        except Exception as e:
            logger.error(f"è§£æé“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _crawl_article(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« è¯¦æƒ…

        æ‰§è¡Œæ­¥éª¤ï¼š
        - éšæœºå»¶è¿Ÿï¼ˆåçˆ¬ç­–ç•¥ï¼‰
        - è·å–å¹¶ç¼“å­˜åŸå§‹é¡µé¢
        - è§£æç»“æ„åŒ–æ•°æ®

        Args:
            url (str): æ–‡ç« è¯¦æƒ…é¡µURL

        Returns:
            dict/None: è§£ææˆåŠŸçš„æ–‡ç« æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        self.random_delay()  # éšæœºå»¶è¿Ÿï¼ˆç»§æ‰¿è‡ªBaseSpiderï¼‰
        logger.info(f"â³ å¼€å§‹çˆ¬å–æ–‡ç« : {url}")

        content = self.fetcher.fetch_and_save(url, direction="CNN", save_origin=True)
        return self._parse_article(content, url) if content else None

    def _parse_article(self, content, url):
        """è§£ææ–‡ç« å†…å®¹ï¼ˆç»“æ„åŒ–ä¼˜å…ˆï¼Œé™çº§è§£æï¼‰

        è§£æç­–ç•¥ï¼š
        1. ä¼˜å…ˆä»JSON-LDè·å–ç»“æ„åŒ–æ•°æ®
        2. å¤±è´¥æ—¶é™çº§ä½¿ç”¨XPathæå–
        3. å†…å®¹æ®µè½æ™ºèƒ½åˆå¹¶

        Args:
            content (str): HTMLå†…å®¹
            url (str): å½“å‰æ–‡ç« URL

        Returns:
            dict: åŒ…å«æ ‡é¢˜ã€ä½œè€…ç­‰å­—æ®µçš„æ–‡ç« æ•°æ®å­—å…¸
        """
        html = etree.HTML(content)
        article = {
            "title": "No Title",
            "url": url,
            "author": "Unknown",
            "publish_time": "",
            "content": ""
        }

        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šç»“æ„åŒ–æ•°æ®ï¼ˆSchema.orgï¼‰
        ld_json = self._extract_json_ld(html)
        if ld_json:
            article.update({
                "title": ld_json.get("headline", "").strip(),
                "author": self._parse_authors(ld_json.get("author", [])),
                "publish_time": ld_json.get("datePublished", "").strip()
            })

        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šHTMLåŸç”Ÿè§£æ
        if not article["title"]:
            title_nodes = html.xpath('//h1/text()')
            article["title"] = title_nodes[0].strip() if title_nodes else "No Title"

        # å†…å®¹æå–ç­–ç•¥
        content_paragraphs = [
            ' '.join(p.xpath('.//text()')).strip()  # åˆå¹¶æ®µè½å†…æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
            for p in html.xpath('//div[contains(@class, "article__content")]//p')
            if p.xpath('.//text()')  # è¿‡æ»¤ç©ºæ®µè½
        ]
        article["content"] = "\n\n".join(content_paragraphs)

        return article

    def _extract_json_ld(self, html):
        """æå–JSON-LDç»“æ„åŒ–æ•°æ®

        JSON-LDè¯´æ˜ï¼š
        - ç½‘ç«™ä½¿ç”¨çš„ç»“æ„åŒ–æ•°æ®æ ¼å¼
        - åŒ…å«æ–‡ç« æ ‡é¢˜ã€ä½œè€…ç­‰å…ƒæ•°æ®
        - å¯èƒ½ä¸ºå•ä¸ªå¯¹è±¡æˆ–å¯¹è±¡æ•°ç»„

        Args:
            html (etree.Element): è§£æåçš„HTMLæ ‘

        Returns:
            dict/None: æˆåŠŸè§£æçš„NewsArticleæ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        for script in html.xpath('//script[@type="application/ld+json"]/text()'):
            try:
                data = json.loads(script.strip())
                # å¤„ç†æ•°ç»„å½¢å¼çš„JSON-LD
                if isinstance(data, list):
                    data = next((item for item in data if item.get("@type") == "NewsArticle"), None)
                # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                if data and data.get("@type") == "NewsArticle":
                    return data
            except json.JSONDecodeError:
                continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„JSON
        return None

    def _parse_authors(self, authors):
        """è§£æä½œè€…ä¿¡æ¯

        å¤„ç†å¤šç§æ•°æ®æ ¼å¼ï¼š
        - å•ä¸ªä½œè€…å¯¹è±¡ï¼š{"name": "xxx"}
        - ä½œè€…æ•°ç»„ï¼š[{"name": "xxx"}, ...]
        - å­—ç¬¦ä¸²æ ¼å¼ï¼š"name"

        Args:
            authors (dict/list/str): åŸå§‹ä½œè€…æ•°æ®

        Returns:
            str: é€—å·åˆ†éš”çš„ä½œè€…åå­—ç¬¦ä¸²
        """
        # ç»Ÿä¸€æ•°æ®æ ¼å¼ä¸ºåˆ—è¡¨
        if isinstance(authors, dict):
            authors = [authors]

        return ", ".join([
            auth.get("name", "").strip()
            for auth in authors
            if isinstance(auth, dict)  # è¿‡æ»¤æ— æ•ˆæ•°æ®
        ]) or "Unknown"  # ç©ºå€¼ä¿æŠ¤

    def _save_results(self, data):
        """æŒä¹…åŒ–å­˜å‚¨è§£æç»“æœ

        å­˜å‚¨ç­–ç•¥ï¼š
        - ä¿å­˜åˆ°parsed/cnnç›®å½•
        - åŒæ—¶ç”ŸæˆCSVå’ŒJSONæ–‡ä»¶
        - æ’é™¤å¤§æ–‡æœ¬å­—æ®µï¼ˆcontentï¼‰çš„CSVè¾“å‡º

        Args:
            data (list): æ–‡ç« æ•°æ®å­—å…¸åˆ—è¡¨
        """
        Saver.save_data(
            data,
            save_dir=f"parsed/{self.name}",  # æŒ‰çˆ¬è™«åç§°åˆ†ç›®å½•
            exclude_columns=['content'],  # CSVæ’é™¤é•¿æ–‡æœ¬
            format_type='both'  # åŒæ—¶ç”Ÿæˆcsvå’Œjson
        )
