# biqu_Spider.py
import csv
import re
from datetime import datetime
from pathlib import Path
from typing import Optional
from urllib.parse import urljoin
from lxml import etree

from utils.BaseSpider import BaseSpider
from utils.TqdmLogHandler import logger
from utils.WebUtils import WebUtils


class biquSpider(BaseSpider):
    """22ç¬”è¶£å°è¯´ç« èŠ‚çˆ¬è™«

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è‡ªåŠ¨ç¿»é¡µå¼ç« èŠ‚æŠ“å–
    - å°è¯´æ­£æ–‡å†…å®¹è§£æ
    - ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹URL
    - ç« èŠ‚æ•°é‡æ§åˆ¶

    ç½‘ç«™ç‰¹æ€§é€‚é…ï¼š
    - ç§»åŠ¨ç«¯é¡µé¢é€‚é…ï¼ˆm.22biqu.comï¼‰
    - ä¸‹ä¸€é¡µæŒ‰é’®ç‰¹æ®Šå®šä½ï¼ˆid="pt_next"ï¼‰
    - ç« èŠ‚å†…å®¹å®¹å™¨å®šä½ï¼ˆid="chaptercontent"ï¼‰

    å…¸å‹æ•°æ®æµï¼š
    èµ·å§‹URL -> è§£æå†…å®¹ -> è·å–ä¸‹ä¸€é¡µ -> å¾ªç¯ç›´è‡³å®Œç»“
    """

    def __init__(self, config=None):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹

        Args:
            config (dict, optional): çˆ¬è™«é…ç½®å‚æ•°ï¼Œå¯è¦†ç›–é»˜è®¤è®¾ç½®
        """
        self.base_dir = "22biqu"
        super().__init__(f"{self.base_dir}", config)  # ç»§æ‰¿åŸºç±»é…ç½®
        self.base_url = "https://m.22biqu.com/biqu5403/5419628.html"  # åˆå§‹ç« èŠ‚URL
        self.visited_urls = set()  # å·²è®¿é—®URLé›†åˆï¼ˆå½“å‰æœªå¯ç”¨ï¼‰

        self.timeout = self.config.get("timeout", 10)
        self.current_url = self.base_url

        # æ–­ç‚¹ç»­ä¼ 
        self.csv_file = Path(f"parsed/{self.base_dir}/crawl_records.csv")
        self._init_csv()
        self.load_processed_urls()
        self.sourcefile = None

    def _extract_links(self, content, current_url):
        """è§£æä¸‹ä¸€é¡µé“¾æ¥

        å®šä½ç­–ç•¥ï¼š
        - é€šè¿‡ç‰¹å®šæŒ‰é’®å®šä½ï¼ˆåŒ…å«idå’Œclasså¤åˆç‰¹å¾ï¼‰
        - ä½¿ç”¨urljoinå¤„ç†ç›¸å¯¹è·¯å¾„

        Args:
            content (str): å½“å‰é¡µé¢HTMLå†…å®¹
            current_url (str): å½“å‰é¡µé¢å®Œæ•´URL

        Returns:
            list: æ ‡å‡†åŒ–åçš„ä¸‹ä¸€é¡µURLåˆ—è¡¨ï¼ˆé€šå¸¸åŒ…å«0-1ä¸ªå…ƒç´ ï¼‰

        Raises:
            Exception: è§£æå¼‚å¸¸æ—¶è®°å½•é”™è¯¯æ—¥å¿—
        """
        try:
            html = etree.HTML(content)
            # ç²¾å‡†å®šä½ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆå¤åˆé€‰æ‹©å™¨ï¼‰
            raw_links = html.xpath('//a[@id="pt_next"][contains(@class,"Readpage_up")]/@href')
            # ç”Ÿæˆç»å¯¹URLï¼ˆå¤„ç†åˆ†é¡µå‚æ•°ï¼‰
            return [urljoin(current_url, link) for link in raw_links]
        except Exception as e:
            logger.error(f"è§£æé“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _extract_article(self, content):
        """è§£æå•ç« å°è¯´å†…å®¹

        æå–è§„åˆ™ï¼š
        - æ ‡é¢˜ä»h1æ ‡ç­¾ç›´æ¥è·å–
        - æ­£æ–‡å†…å®¹åˆå¹¶æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        - ä¿ç•™å½“å‰URLä½œä¸ºæ•°æ®æº¯æº

        Args:
            content (str): ç« èŠ‚é¡µé¢HTMLå†…å®¹

        Returns:
            dict/None: åŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰å­—æ®µçš„å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        try:
            html = etree.HTML(content)
            return {
                "title": html.xpath('//h1/text()')[0].strip(),  # æ ‡é¢˜å¿…å¡«é¡¹
                "content": '\n'.join(
                    html.xpath('//div[@id="chaptercontent"]//text()')  # åˆå¹¶æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                ).strip(),
                "url": self.current_url  # è®°å½•æ•°æ®æ¥æº
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}")
            return None

    def crawl(self, max_articles=50):
        """é“¾å¼ç« èŠ‚æŠ“å–ä¸»æµç¨‹

        æ‰§è¡Œé€»è¾‘ï¼š
        1. ä»åˆå§‹URLå¼€å§‹å¾ªç¯
        2. è·å–å¹¶è§£æå½“å‰ç« èŠ‚
        3. å®šä½ä¸‹ä¸€é¡µé“¾æ¥
        4. æ»¡è¶³ç»ˆæ­¢æ¡ä»¶æ—¶é€€å‡ºï¼ˆè¾¾åˆ°æœ€å¤§æ•°é‡æˆ–æ— åç»­ç« èŠ‚ï¼‰

        Args:
            max_articles (int): æœ€å¤§æŠ“å–ç« èŠ‚æ•°ï¼Œé»˜è®¤50ç« 
        """
        current_url = self.base_url

        while max_articles > 0 and current_url:
            # é˜²é‡å¤æœºåˆ¶ï¼ˆå½“å‰æ³¨é‡ŠçŠ¶æ€ï¼Œéœ€è¦æ—¶å¯å¯ç”¨ï¼‰
            if current_url in self.visited_urls:
                break

            # è·å–å¹¶ç¼“å­˜åŸå§‹é¡µé¢
            content = self.fetcher.fetch_and_save(current_url, direction=f"{self.base_dir}", save_origin=True)

            # è·å–ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆé€šå¸¸åŒ…å«0-1ä¸ªå…ƒç´ ï¼‰
            next_links = self._extract_links(content, current_url)
            if not next_links:
                logger.info("ğŸ›‘ å·²åˆ°è¾¾æœ€ç»ˆç« èŠ‚")
                break

            # æ›´æ–°å½“å‰URLï¼ˆå®ç°é“¾å¼è·³è½¬ï¼‰
            current_url = next_links[0]

    def _init_csv(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´"""
        # åˆ›å»ºçˆ¶ç›®å½•ï¼ˆè‡ªåŠ¨é€’å½’åˆ›å»ºï¼‰
        self.csv_file.parent.mkdir(parents=True, exist_ok=True)
        if not self.csv_file.exists():
            with open(self.csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=[
                    'timestamp',
                    'url',
                    'source_file',
                    'parsed_file',
                    'book_name',
                    'chapter_name',
                    'status'
                ])
                writer.writeheader()

    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URL"""
        self.processed_urls = set()
        try:
            with open(self.csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row['url'] and row['status'] == 'success':
                        # self.processed_urls.add(row['url'])
                        self.visited_urls.add(row['url'])
        except FileNotFoundError:
            pass

    def _update_csv(self, record: dict):
        """æ›´æ–°CSVè®°å½•"""
        try:
            with open(self.csv_file, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=record.keys())
                writer.writerow(record)
        except Exception as e:
            logger.error(f"CSVè®°å½•æ›´æ–°å¤±è´¥: {str(e)}")

    def _get_source_file(self, url: str, direction: str, file_name = None):
        """è®°å½•åŸå§‹æ–‡ä»¶ä¿¡æ¯"""
        if not file_name:
            file_name = WebUtils.generate_filename(url)
        self.sourcefile = Path("origin") / direction / file_name

    def fetch_content(self, url: str, direction: str, file_name: str = None) -> Optional[str]:
        """ç»Ÿä¸€å°è£…çš„å†…å®¹è·å–æ–¹æ³•"""
        if url in self.visited_urls:
            logger.warning(f"â© è·³è¿‡å·²ä¿å­˜URL: {url}")
            return url

        self.visited_urls.add(url)
        self.random_delay()
        logger.info(f"ğŸ“– è®¿é—®URL: {url}")
        content = self.fetcher.fetch_and_save(
            url=url,
            direction=direction,
            file_name=file_name,
            save_origin=True
        )
        if content:
            self._get_source_file(url, direction, file_name)
        if not content:
            logger.error(f"ğŸ›‘ è·å–å†…å®¹å¤±è´¥: {url}")
        return content

    def _save_chapter_data(self, book_name: str, chapter_name: str,
                           chapter_url: str, content: str) -> None:
        """ç»Ÿä¸€çš„ç« èŠ‚ä¿å­˜æ–¹æ³•"""
        try:
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            safe_book_name = re.sub(r'[\\/*?:"<>|]', '', book_name)[:50]
            safe_chapter_name = re.sub(r'[\\/*?:"<>|]', '', chapter_name)[:50]

            # åˆ›å»ºå­˜å‚¨è·¯å¾„
            save_dir = Path("parsed") / safe_book_name
            save_dir.mkdir(parents=True, exist_ok=True)

            # å†™å…¥æ–‡ä»¶å†…å®¹
            file_path = save_dir / f"{safe_chapter_name}.txt"
            with file_path.open("w", encoding="utf-8-sig") as f:
                f.write(f"Book: {book_name}\n")
                f.write(f"Chapter: {chapter_name}\n")
                f.write(f"URL: {chapter_url}\n\n")
                f.write(content)

            # è®°å½•è§£ææ–‡ä»¶ä¿¡æ¯
            self._update_csv({
                'timestamp': datetime.now().isoformat(),
                'url': chapter_url,
                'source_file': str(self.sourcefile),
                'parsed_file': str(file_path),
                'book_name': book_name,
                'chapter_name': chapter_name,
                'status': "success"
            })

            logger.info(f"âœ… æˆåŠŸä¿å­˜ç« èŠ‚: {file_path}")
        except Exception as e:
            logger.error(f"ğŸ›‘ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}", exc_info=True)