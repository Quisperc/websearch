# biqu_Spider.py
from urllib.parse import urljoin
from lxml import etree

from utils.BaseSpider import BaseSpider
from utils.TqdmLogHandler import logger


class biquSpider(BaseSpider):
    """22ç¬”è¶£å°è¯´ç« èŠ‚çˆ¬è™«

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è‡ªåŠ¨ç¿»é¡µå¼ç« èŠ‚æŠ“å–
    - å°è¯´æ­£æ–‡å†…å®¹è§£æ
    - ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹URL
    - ç« èŠ‚æ•°é‡æ§åˆ¶

    ç½‘ç«™ç‰¹æ€§é€‚é…ï¼š
    - ç§»åŠ¨ç«¯é¡µé¢é€‚é…ï¼ˆm.22biqu.comï¼‰
    - ä¸‹ä¸€é¡µæŒ‰é’®ç‰¹æ®Šå®šä½ï¼ˆid="pt_next"ï¼‰
    - ç« èŠ‚å†…å®¹å®¹å™¨å®šä½ï¼ˆid="chaptercontent"ï¼‰

    å…¸å‹æ•°æ®æµï¼š
    èµ·å§‹URL -> è§£æå†…å®¹ -> è·å–ä¸‹ä¸€é¡µ -> å¾ªç¯ç›´è‡³å®Œç»“
    """

    def __init__(self, config=None):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹

        Args:
            config (dict, optional): çˆ¬è™«é…ç½®å‚æ•°ï¼Œå¯è¦†ç›–é»˜è®¤è®¾ç½®
        """
        super().__init__("22biqu", config)  # ç»§æ‰¿åŸºç±»é…ç½®
        self.base_url = "https://m.22biqu.com/biqu5403/5419628.html"  # åˆå§‹ç« èŠ‚URL
        self.visited_urls = set()  # å·²è®¿é—®URLé›†åˆï¼ˆå½“å‰æœªå¯ç”¨ï¼‰

    def _extract_links(self, content, current_url):
        """è§£æä¸‹ä¸€é¡µé“¾æ¥

        å®šä½ç­–ç•¥ï¼š
        - é€šè¿‡ç‰¹å®šæŒ‰é’®å®šä½ï¼ˆåŒ…å«idå’Œclasså¤åˆç‰¹å¾ï¼‰
        - ä½¿ç”¨urljoinå¤„ç†ç›¸å¯¹è·¯å¾„

        Args:
            content (str): å½“å‰é¡µé¢HTMLå†…å®¹
            current_url (str): å½“å‰é¡µé¢å®Œæ•´URL

        Returns:
            list: æ ‡å‡†åŒ–åçš„ä¸‹ä¸€é¡µURLåˆ—è¡¨ï¼ˆé€šå¸¸åŒ…å«0-1ä¸ªå…ƒç´ ï¼‰

        Raises:
            Exception: è§£æå¼‚å¸¸æ—¶è®°å½•é”™è¯¯æ—¥å¿—
        """
        try:
            html = etree.HTML(content)
            # ç²¾å‡†å®šä½ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆå¤åˆé€‰æ‹©å™¨ï¼‰
            raw_links = html.xpath('//a[@id="pt_next"][contains(@class,"Readpage_up")]/@href')
            # ç”Ÿæˆç»å¯¹URLï¼ˆå¤„ç†åˆ†é¡µå‚æ•°ï¼‰
            return [urljoin(current_url, link) for link in raw_links]
        except Exception as e:
            logger.error(f"è§£æé“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _extract_article(self, content):
        """è§£æå•ç« å°è¯´å†…å®¹

        æå–è§„åˆ™ï¼š
        - æ ‡é¢˜ä»h1æ ‡ç­¾ç›´æ¥è·å–
        - æ­£æ–‡å†…å®¹åˆå¹¶æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        - ä¿ç•™å½“å‰URLä½œä¸ºæ•°æ®æº¯æº

        Args:
            content (str): ç« èŠ‚é¡µé¢HTMLå†…å®¹

        Returns:
            dict/None: åŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰å­—æ®µçš„å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        try:
            html = etree.HTML(content)
            return {
                "title": html.xpath('//h1/text()')[0].strip(),  # æ ‡é¢˜å¿…å¡«é¡¹
                "content": '\n'.join(
                    html.xpath('//div[@id="chaptercontent"]//text()')  # åˆå¹¶æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                ).strip(),
                "url": self.current_url  # è®°å½•æ•°æ®æ¥æº
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}")
            return None

    def crawl(self, max_articles=50):
        """é“¾å¼ç« èŠ‚æŠ“å–ä¸»æµç¨‹

        æ‰§è¡Œé€»è¾‘ï¼š
        1. ä»åˆå§‹URLå¼€å§‹å¾ªç¯
        2. è·å–å¹¶è§£æå½“å‰ç« èŠ‚
        3. å®šä½ä¸‹ä¸€é¡µé“¾æ¥
        4. æ»¡è¶³ç»ˆæ­¢æ¡ä»¶æ—¶é€€å‡ºï¼ˆè¾¾åˆ°æœ€å¤§æ•°é‡æˆ–æ— åç»­ç« èŠ‚ï¼‰

        Args:
            max_articles (int): æœ€å¤§æŠ“å–ç« èŠ‚æ•°ï¼Œé»˜è®¤50ç« 
        """
        current_url = self.base_url

        while max_articles > 0 and current_url:
            # é˜²é‡å¤æœºåˆ¶ï¼ˆå½“å‰æ³¨é‡ŠçŠ¶æ€ï¼Œéœ€è¦æ—¶å¯å¯ç”¨ï¼‰
            # if current_url in self.visited_urls:
            #     break

            # è·å–å¹¶ç¼“å­˜åŸå§‹é¡µé¢
            content = self.fetcher.fetch_and_save(current_url, language="22biqu", save_origin=True)

            # æ•°æ®ä¿å­˜é€»è¾‘ï¼ˆå½“å‰æ³¨é‡ŠçŠ¶æ€ï¼ŒæŒ‰éœ€å¯ç”¨ï¼‰
            # if article := self._extract_article(content):
            #     Saver.save_data([article])
            #     max_articles -= 1
            #     logger.info(f"å‰©ä½™ç« èŠ‚æ•°: {max_articles} | å½“å‰ç« èŠ‚: {article['title']}")

            # è·å–ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆé€šå¸¸åŒ…å«0-1ä¸ªå…ƒç´ ï¼‰
            next_links = self._extract_links(content, current_url)
            if not next_links:
                logger.info("ğŸ›‘ å·²åˆ°è¾¾æœ€ç»ˆç« èŠ‚")
                break

            # æ›´æ–°å½“å‰URLï¼ˆå®ç°é“¾å¼è·³è½¬ï¼‰
            current_url = next_links[0]
            # self.visited_urls.add(current_url)  # å»é‡æœºåˆ¶ï¼ˆéœ€é…åˆå¯ç”¨é˜²é‡å¤åˆ¤æ–­ï¼‰
