# biqu_Spider.py
import csv
import re
from datetime import datetime
from pathlib import Path
from typing import Optional
from urllib.parse import urljoin

import pandas as pd
from bs4 import BeautifulSoup

from utils.BaseSpider import BaseSpider
from utils.TqdmLogHandler import logger
from utils.WebUtils import WebUtils
# å¤„ç†
from utils.dealer_cn import dealer_cn


class biquSpider(BaseSpider):
    """22ç¬”è¶£å°è¯´ç« èŠ‚çˆ¬è™«

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è‡ªåŠ¨ç¿»é¡µå¼ç« èŠ‚æŠ“å–
    - å°è¯´æ­£æ–‡å†…å®¹è§£æ
    - ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹URL
    - ç« èŠ‚æ•°é‡æ§åˆ¶

    ç½‘ç«™ç‰¹æ€§é€‚é…ï¼š
    - ç§»åŠ¨ç«¯é¡µé¢é€‚é…ï¼ˆm.22biqu.comï¼‰
    - ä¸‹ä¸€é¡µæŒ‰é’®ç‰¹æ®Šå®šä½ï¼ˆid="pt_next"ï¼‰
    - ç« èŠ‚å†…å®¹å®¹å™¨å®šä½ï¼ˆid="chaptercontent"ï¼‰

    å…¸å‹æ•°æ®æµï¼š
    èµ·å§‹URL -> è§£æå†…å®¹ -> è·å–ä¸‹ä¸€é¡µ -> å¾ªç¯ç›´è‡³å®Œç»“
    """

    def __init__(self, config=None):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹

        Args:
            config (dict, optional): çˆ¬è™«é…ç½®å‚æ•°ï¼Œå¯è¦†ç›–é»˜è®¤è®¾ç½®
        """
        self.base_dir = "22biqu"
        super().__init__(f"{self.base_dir}", config)  # ç»§æ‰¿åŸºç±»é…ç½®
        self.base_url = "https://m.22biqu.com/biqu5403/5419018.html"  # åˆå§‹ç« èŠ‚URL
        # self.visited_urls = set()  # å·²è®¿é—®URLé›†åˆ
        self.visited_urls = []  # æ”¹ç”¨åˆ—è¡¨

        self.timeout = self.config.get("timeout", 10)
        self.current_url = self.base_url
        self.current_page = 1

        # æ–­ç‚¹ç»­ä¼ 
        #self.csv_file = Path(f"parsed/{self.base_dir}/biqi_crawl_records.csv")
        self.csv_file = Path(f"parsed/biqi_crawl_records.csv")
        self._init_csv()
        self.load_processed_urls()
        self.sourcefile = None

    def _extract_links(self, content, current_url):
        """è§£æä¸‹ä¸€é¡µé“¾æ¥

        å®šä½ç­–ç•¥ï¼š
        - é€šè¿‡ç‰¹å®šæŒ‰é’®å®šä½ï¼ˆåŒ…å«idå’Œclasså¤åˆç‰¹å¾ï¼‰
        - ä½¿ç”¨urljoinå¤„ç†ç›¸å¯¹è·¯å¾„

        Args:
            content (str): å½“å‰é¡µé¢HTMLå†…å®¹
            current_url (str): å½“å‰é¡µé¢å®Œæ•´URL

        Returns:
            list: æ ‡å‡†åŒ–åçš„ä¸‹ä¸€é¡µURLåˆ—è¡¨ï¼ˆé€šå¸¸åŒ…å«0-1ä¸ªå…ƒç´ ï¼‰

        Raises:
            Exception: è§£æå¼‚å¸¸æ—¶è®°å½•é”™è¯¯æ—¥å¿—
        """
        try:
            soup = BeautifulSoup(content, 'lxml')
            # ç²¾å‡†å®šä½ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆå¤åˆé€‰æ‹©å™¨
            raw_links = soup.select_one('a#pt_next.Readpage_up')['href']
            # ç”Ÿæˆç»å¯¹URLï¼ˆå¤„ç†åˆ†é¡µå‚æ•°ï¼‰
            # return [urljoin(current_url, link) for link in raw_links]
            return [urljoin(current_url, raw_links)]
        except Exception as e:
            logger.error(f"è§£æé“¾æ¥å¤±è´¥: {str(e)}")
            return None

    def _extract_article(self, content, page = 0):
        """è§£æå•ç« å°è¯´å†…å®¹

        æå–è§„åˆ™ï¼š
        - æ­£æ–‡å†…å®¹åˆå¹¶æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        - ä¿ç•™å½“å‰URLä½œä¸ºæ•°æ®æº¯æº

        Args:
            content (str): ç« èŠ‚é¡µé¢HTMLå†…å®¹

        Returns:
            dict/None: åŒ…å«æ ‡é¢˜ã€å†…å®¹ç­‰å­—æ®µçš„å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        try:
            soup = BeautifulSoup(content, 'lxml')
            # è·å–ç« èŠ‚å
            # å®šä½åˆ°ç›®æ ‡æ ‡ç­¾
            meta_tag = soup.find('meta', attrs={'name': 'keywords'})
            book_name = "ã€Šè¯¡ç§˜ä¹‹ä¸»ã€‹"
            chapter_name = "Unknown Chapter"
            if meta_tag:
                # è·å–contentå±æ€§çš„å€¼
                content_str = meta_tag['content']
                # åˆ†å‰²å¹¶æå–ç›®æ ‡éƒ¨åˆ†
                # å–ä¹¦å
                book_name = content_str.split(',', 1)[0]
                # 1. æŒ‰é€—å·åˆ†å‰²ï¼Œå–ç¬¬äºŒéƒ¨åˆ†
                second_part = content_str.split(',', 1)[1]  # 1è¡¨ç¤ºåªåˆ†å‰²ä¸€æ¬¡
                # 2. æŒ‰å¥å·åˆ†å‰²ï¼Œå–ç¬¬äºŒéƒ¨åˆ†
                self.current_page = second_part.split('.',1)[0].strip()
                chapter_name = second_part.split('.', 1)[1].strip()

            if not page:
                # å…¶ä½™Pæ ‡ç­¾ä½œä¸ºcontext
                content_elems = soup.select('#chaptercontent p:not(:first-child)')
                content_text = "\n".join([elem.get_text(strip=True) for elem in content_elems])
                chapter_name = chapter_name + f"ç¬¬{page + 1}é¡µ"
            else:
                # å…¶ä½™Pæ ‡ç­¾ä½œä¸ºcontext
                content_elems = soup.select('#chaptercontent p')
                content_text = "\n".join([elem.get_text(strip=True) for elem in content_elems])
                chapter_name = chapter_name + f"ç¬¬{page + 1}é¡µ"

            self._save_chapter_data(
                book_name=book_name,
                chapter_name=chapter_name,
                chapter_url=self.current_url,
                content=content_text
            )

            # å¤„ç†ç« èŠ‚å†…å®¹
            # å•è¯å­—ç¬¦åŒ–ã€åˆ é™¤ç‰¹æ®Šå­—ç¬¦ã€å¤§å°å†™è½¬æ¢
            text = self.dealer.clean_text(content_text)
            self.dealer._save_chapter_data(
                book_name=book_name,
                chapter_name= chapter_name,
                content= text)

            return {
                "chapter_page": self.current_page,
                "book_name":book_name,
                "chapter_name":chapter_name,
                "chapter_url":self.current_url,
                "content":content_text
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}")
            return None

    def crawl(self, max_articles=50):
        """é“¾å¼ç« èŠ‚æŠ“å–ä¸»æµç¨‹

        æ‰§è¡Œé€»è¾‘ï¼š
        1. ä»åˆå§‹URLå¼€å§‹å¾ªç¯
        2. è·å–å¹¶è§£æå½“å‰ç« èŠ‚
        3. å®šä½ä¸‹ä¸€é¡µé“¾æ¥
        4. æ»¡è¶³ç»ˆæ­¢æ¡ä»¶æ—¶é€€å‡ºï¼ˆè¾¾åˆ°æœ€å¤§æ•°é‡æˆ–æ— åç»­ç« èŠ‚ï¼‰

        Args:
            max_articles (int): æœ€å¤§æŠ“å–ç« èŠ‚æ•°ï¼Œé»˜è®¤50ç« 
        """
        current_num = 0
        try:
            if not self.visited_urls:
                self.current_url = self.base_url
            else:
                self.current_url = self.visited_urls[-1]
                self.visited_urls.pop(-1)
                # åˆ é™¤csvæ–‡ä»¶æœ€åä¸€æ¡æ•°æ®
                self.delete_row_by_url(self.current_url)
                current_num = current_num + len(self.visited_urls)
            while max_articles > current_num and self.current_url:
                # è·å–å¹¶ç¼“å­˜åŸå§‹é¡µé¢
                page2 = current_num % 2
                page1 = int(current_num / 2)
                content = self.fetch_content(
                    self.current_url,
                    direction=f"{self.base_dir}",
                    file_name=f"ã€Šè¯¡ç§˜ä¹‹ä¸»ã€‹-ç¬¬{page1 + 1}ç« ç¬¬{page2 + 1}é¡µ.html"
                )
                if content is None:
                    # è·å–ä¸‹ä¸€ä¸ªé“¾æ¥
                    current_num += 1
                    continue
                # è§£ææ­£æ–‡
                self._extract_article(content,page= page2)
                current_num += 1
                # è·å–ä¸‹ä¸€é¡µé“¾æ¥ï¼ˆé€šå¸¸åŒ…å«0-1ä¸ªå…ƒç´ ï¼‰
                next_links = self._extract_links(content, self.current_url)
                if not next_links:
                    logger.info("ğŸ›‘ å·²åˆ°è¾¾æœ€ç»ˆç« èŠ‚")
                    break

                # æ›´æ–°å½“å‰URLï¼ˆå®ç°é“¾å¼è·³è½¬ï¼‰
                self.current_url = next_links[0]
        except KeyboardInterrupt:
            logger.warning("â½¤æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            logger.error(f"çˆ¬å–æµç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            logger.info(f"ğŸ‰ å®Œæˆå¤„ç† {int((current_num+1)/2)}/{int((max_articles+1)/2)} ç« ")

    def _init_csv(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´"""
        # åˆ›å»ºçˆ¶ç›®å½•ï¼ˆè‡ªåŠ¨é€’å½’åˆ›å»ºï¼‰
        self.csv_file.parent.mkdir(parents=True, exist_ok=True)
        if not self.csv_file.exists():
            with open(self.csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=[
                    'timestamp',
                    'url',
                    'source_file',
                    'parsed_file',
                    'book_name',
                    'chapter_name',
                    'status'
                ])
                writer.writeheader()

    def load_processed_urls(self):
        """åŠ è½½å·²å¤„ç†çš„URL"""
        # self.processed_urls = set()
        self.visited_urls = []
        try:
            with open(self.csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row['url'] and row['status'] == 'success':
                        # self.processed_urls.add(row['url'])
                        self.visited_urls.append(row['url'])
        except FileNotFoundError:
            pass

    def _update_csv(self, record: dict):
        """æ›´æ–°CSVè®°å½•"""
        try:
            with open(self.csv_file, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=record.keys())
                writer.writerow(record)
        except Exception as e:
            logger.error(f"CSVè®°å½•æ›´æ–°å¤±è´¥: {str(e)}")

    def _get_source_file(self, url: str, direction: str, file_name = None):
        """è®°å½•åŸå§‹æ–‡ä»¶ä¿¡æ¯"""
        if not file_name:
            file_name = WebUtils.generate_filename(url)
        self.sourcefile = Path("origin") / direction / file_name

    def fetch_content(self, url: str, direction: str, file_name: str = None) -> Optional[str]:
        """ç»Ÿä¸€å°è£…çš„å†…å®¹è·å–æ–¹æ³•"""
        if url in self.visited_urls:
            logger.warning(f"â© è·³è¿‡å·²ä¿å­˜URL: {url}")
            return None

        # self.visited_urls.add(url)
        self.visited_urls.append(url)  # æŒ‰é¡ºåºè®°å½•
        self.random_delay()
        logger.info(f"ğŸ“– è®¿é—®URL: {url}")
        content = self.fetcher.fetch_and_save(
            url=url,
            direction=direction,
            file_name=file_name,
            save_origin=True
        )
        if content:
            self._get_source_file(url, direction, file_name)
        if not content:
            logger.error(f"ğŸ›‘ è·å–å†…å®¹å¤±è´¥: {url}")
        return content

    def _save_chapter_data(self, book_name: str, chapter_name: str,
                           chapter_url: str, content: str) -> None:
        """ç»Ÿä¸€çš„ç« èŠ‚ä¿å­˜æ–¹æ³•"""
        try:
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            safe_book_name = re.sub(r'[\\/*?:"<>|]', '', book_name)[:50]
            safe_chapter_name = re.sub(r'[\\/*?:"<>|]', '', chapter_name)[:50]

            # åˆ›å»ºå­˜å‚¨è·¯å¾„
            save_dir = Path("parsed") / safe_book_name
            save_dir.mkdir(parents=True, exist_ok=True)

            # å†™å…¥æ–‡ä»¶å†…å®¹
            file_path = save_dir / f"{safe_chapter_name}.txt"
            with file_path.open("w", encoding="utf-8-sig") as f:
                f.write(f"Book: {book_name}\n")
                f.write(f"Chapter: {chapter_name}\n")
                f.write(f"URL: {chapter_url}\n\n")
                f.write(content)

            # è®°å½•è§£ææ–‡ä»¶ä¿¡æ¯
            self._update_csv({
                'timestamp': datetime.now().isoformat(),
                'url': chapter_url,
                'source_file': str(self.sourcefile),
                'parsed_file': str(file_path),
                'book_name': book_name,
                'chapter_name': chapter_name,
                'status': "success"
            })

            logger.info(f"âœ… æˆåŠŸä¿å­˜ç« èŠ‚: {file_path}")
        except Exception as e:
            logger.error(f"ğŸ›‘ æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}", exc_info=True)

    def delete_row_by_url(self, current_url):
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(self.csv_file, encoding='utf-8-sig')

        # è¿‡æ»¤æ‰urlåˆ—ç­‰äºcurrent_urlçš„è¡Œ
        df = df[df['url'] != current_url]

        # ä¿å­˜ç»“æœï¼ˆè¦†ç›–åŸæ–‡ä»¶æˆ–ä¿å­˜ä¸ºæ–°æ–‡ä»¶ï¼‰
        df.to_csv(self.csv_file, encoding='utf-8-sig', index=False)  # è¦†ç›–åŸæ–‡ä»¶
        # df.to_csv('new_file.csv', index=False)  # ä¿å­˜ä¸ºæ–°æ–‡ä»¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰