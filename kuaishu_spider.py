# kuaishu_spider.py
from utils.BaseSpider import BaseSpider
from utils.Saver import Saver
from utils.TqdmLogHandler import logger


class kuaishuSpider(BaseSpider):
    """å¿«ä¹¦å°è¯´çˆ¬è™«

    æ ¸å¿ƒç‰¹æ€§ï¼š
    - åˆ†é¡µå¼å°è¯´ç« èŠ‚æŠ“å–
    - å¹¶è¡ŒåŒ–é¡µé¢ä¸‹è½½
    - è‡ªåŠ¨é¡µç ç”Ÿæˆ
    - æ•°æ®æŒä¹…åŒ–å­˜å‚¨

    å…¸å‹ä½¿ç”¨åœºæ™¯ï¼š
    - æ‰¹é‡ä¸‹è½½æŒ‡å®šé¡µç èŒƒå›´çš„å°è¯´ç« èŠ‚
    - åˆ†å¸ƒå¼é‡‡é›†ä»»åŠ¡ç¼–æ’
    - åŸå§‹HTMLä¸è§£æç»“æœçš„åŒé‡å­˜å‚¨

    æ‰©å±•æ–¹å‘ï¼š
    - å®Œå–„_parse_pageè§£æé€»è¾‘
    - å¢åŠ åçˆ¬ç­–ç•¥
    - æ”¯æŒæ–­ç‚¹ç»­çˆ¬
    """

    def __init__(self, config=None):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹

        Args:
            config (dict, optional): çˆ¬è™«é…ç½®å‚æ•°ï¼Œå¯è¦†ç›–ä»¥ä¸‹é»˜è®¤è®¾ç½®ï¼š
                - delay: è¯·æ±‚å»¶è¿Ÿæ—¶é—´
                - retries: é‡è¯•æ¬¡æ•°
                - concurrency: å¹¶å‘æ•°
        """
        super().__init__("novel", config)  # ç»§æ‰¿åŸºç¡€é…ç½®
        # å°è¯´ç« èŠ‚URLæ¨¡æ¿ï¼ˆéœ€æ›¿æ¢é¡µç å‚æ•°ï¼‰
        self.base_url = "https://www.kuaishu5.com/b121721/{}.html"

    def crawl(self, start_page=27710, end_page=27711):
        """ä¸»çˆ¬å–æ§åˆ¶æµç¨‹

        æ‰§è¡Œæ­¥éª¤ï¼š
        1. ç”Ÿæˆé¡µç ä»»åŠ¡åºåˆ—
        2. å¹¶è¡Œæ‰§è¡Œé¡µé¢æŠ“å–
        3. ç»Ÿè®¡å¹¶ä¿å­˜ç»“æœ

        Args:
            start_page (int): èµ·å§‹é¡µç ï¼Œé»˜è®¤27710
            end_page (int): ç»“æŸé¡µç ï¼Œé»˜è®¤27711ï¼ˆåŒ…å«è¯¥é¡µç ï¼‰
        """
        # ç”Ÿæˆè¿ç»­é¡µç ä»»åŠ¡åˆ—è¡¨
        tasks = list(range(start_page, end_page + 1))

        # å¹¶è¡Œæ‰§è¡Œé¡µé¢é‡‡é›†ï¼ˆä½¿ç”¨çˆ¶ç±»çº¿ç¨‹æ± ï¼‰
        results = self.parallel_execute(tasks, self._crawl_page)

        # æ•°æ®æŒä¹…åŒ–ï¼ˆå½“å‰ä¿å­˜åŠŸèƒ½è¢«æ³¨é‡Šï¼‰
        if results:
            # self._save_results(results)
            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(results)} ç« å°è¯´å†…å®¹")
        else:
            logger.info("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")

    def _crawl_page(self, page_num):
        """å•ç« èŠ‚æŠ“å–æµç¨‹

        æ‰§è¡Œæ­¥éª¤ï¼š
        - éšæœºå»¶è¿Ÿï¼ˆåçˆ¬ï¼‰
        - ç”ŸæˆåŠ¨æ€URL
        - ä¸‹è½½å¹¶ç¼“å­˜é¡µé¢
        - è§£ææœ‰æ•ˆå†…å®¹

        Args:
            page_num (int): å½“å‰ç« èŠ‚é¡µç 

        Returns:
            dict/None: è§£æåçš„ç« èŠ‚æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        self.random_delay()  # ç»§æ‰¿è‡ªBaseSpiderçš„éšæœºå»¶è¿Ÿ
        url = self.base_url.format(page_num)
        logger.info(f"ğŸ•¸ï¸ æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")

        # å¸¦è‡ªåŠ¨ç¼“å­˜çš„è¯·æ±‚ï¼ˆåŸå§‹HTMLä¿å­˜è‡³data/novel/rawç›®å½•ï¼‰
        content = self.fetcher.fetch_and_save(url, direction="Novel")
        return self._parse_page(content, url) if content else None

    def _parse_page(self, content, url):
        """é¡µé¢è§£æé€»è¾‘ï¼ˆå¾…å®ç°æ¨¡æ¿ï¼‰

        é¢„æœŸè§£æå†…å®¹ï¼š
        - ç« èŠ‚æ ‡é¢˜
        - æ­£æ–‡å†…å®¹
        - ä½œè€…ä¿¡æ¯
        - æ›´æ–°æ—¶é—´

        Args:
            content (str): é¡µé¢HTMLå†…å®¹
            url (str): å½“å‰é¡µé¢URL

        Returns:
            dict: åŒ…å«æ ‡é¢˜ã€URLå’Œå†…å®¹çš„å­—å…¸ï¼ˆç¤ºä¾‹å®ç°ï¼‰

        TODO:
            éœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„å®Œå–„XPath/cssé€‰æ‹©å™¨
        """
        # ç¤ºä¾‹è§£æé€»è¾‘ï¼ˆéœ€æ›¿æ¢ä¸ºå®é™…ç½‘ç«™ç»“æ„ï¼‰
        return {
            "title": f"ç« èŠ‚{url.split('/')[-1]}",  # ä»URLæå–ç« èŠ‚ID
            "url": url,
            "content": "ç¤ºä¾‹å†…å®¹"  # å¾…æ›¿æ¢ä¸ºçœŸå®è§£æé€»è¾‘
        }

    def _save_results(self, data):
        """æ•°æ®æŒä¹…åŒ–å­˜å‚¨

        å­˜å‚¨ç­–ç•¥ï¼š
        - ä¿å­˜åˆ°parsed/novelç›®å½•
        - ç”ŸæˆExcelæ ¼å¼æ–‡ä»¶
        - è‡ªåŠ¨æ’é™¤åŸå§‹HTMLå­—æ®µ

        Args:
            data (list): è§£æåçš„æ•°æ®å­—å…¸åˆ—è¡¨
        """
        Saver.save_data(
            data,
            save_dir=f"parsed/{self.name}",  # æŒ‰çˆ¬è™«åç§°åˆ†ç›®å½•
            exclude_columns=['raw_html'],  # æ’é™¤åŸå§‹HTMLå­—æ®µ
            format_type='excel'  # ç”ŸæˆExcelæ–‡ä»¶
        )
