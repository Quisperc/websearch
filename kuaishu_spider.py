# kuaishu_spider.py
from utils import BaseSpider, Saver, logger


class kuaishuSpider(BaseSpider):
    """å°è¯´çˆ¬è™«"""

    def __init__(self, config=None):
        super().__init__("novel", config)
        self.base_url = "https://www.kuaishu5.com/b121721/{}.html"

    def crawl(self, start_page=27710, end_page=27711):
        """çˆ¬å–æŒ‡å®šé¡µæ•°èŒƒå›´"""
        tasks = list(range(start_page, end_page + 1))
        results = self.parallel_execute(tasks, self._crawl_page)
        if results:
            # self._save_results(results)
            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(results)} ç« å°è¯´å†…å®¹")
        else:
            logger.info("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")

    def _crawl_page(self, page_num):
        """çˆ¬å–å•ä¸ªç« èŠ‚"""
        self.random_delay()
        url = self.base_url.format(page_num)
        logger.info(f"ğŸ•¸ï¸ æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é¡µ: {url}")

        content = self.fetcher.fetch_and_save(url, language="Novel")
        return self._parse_page(content, url) if content else None

    def _parse_page(self, content, url):
        """è§£æå°è¯´é¡µé¢ï¼ˆç¤ºä¾‹å®ç°ï¼‰"""
        # TODO: æ ¹æ®å®é™…ç½‘ç«™ç»“æ„å®ç°è§£æé€»è¾‘
        return {
            "title": f"ç« èŠ‚{url.split('/')[-1]}",
            "url": url,
            "content": "ç¤ºä¾‹å†…å®¹"
        }

    def _save_results(self, data):
        """ä¿å­˜å°è¯´æ•°æ®"""
        Saver.save_data(
            data,
            save_dir=f"parsed/{self.name}",
            exclude_columns=['raw_html'],
            format_type='excel'
        )
