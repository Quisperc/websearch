# yinyu_Spider.py
import re
from pathlib import Path
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from typing import Optional, List, Dict
from pandas.core.generic import bool_t
from utils.BaseSpider import BaseSpider
from utils.Saver import Saver
from utils.TqdmLogHandler import logger

class yinyuSpider(BaseSpider):
    """è‹±è¯­å°è¯´ç« èŠ‚çˆ¬è™«"""

    def __init__(self, config=None):
        super().__init__("yinyu", config or {})
        self.base_url = "https://www.yingyuxiaoshuo.com/"
        self.visited_urls = set()
        self.timeout = self.config.get("timeout", 10)
        self.current_url = self.base_url  # è·Ÿè¸ªå½“å‰çˆ¬å–URL

    def _extract_books(self, content: str) -> Optional[Dict]:
        """è§£æä¹¦ç±åˆ—è¡¨çš„å¼ºåŒ–ç‰ˆæœ¬"""
        try:
            html = BeautifulSoup(content, 'lxml')
            available_books = []

            # ä½¿ç”¨æ›´ç¨³å¥çš„é€‰æ‹©å™¨
            title = html.find('title').get_text(strip=True) if html.title else "No Title"

            # ä½¿ç”¨CSSé€‰æ‹©å™¨æé«˜å‡†ç¡®æ€§
            # book_items = html.select('h2.inline.text-danger.hover\\:text-hover1.font-semibold a')
            # ç­›é€‰è‹±æ–‡å
            book_items = html.select('h2.inline.italic.text-xs.text-gray1.hover\\:text-hover1.max-sm\\:hidden.ml-2 a')

            for book_link in book_items:
                if not (name := book_link.get_text(strip=True)):
                    continue

                if not (href := book_link.get('href')):
                    continue

                available_books.append({
                    "name": name,
                    "url": urljoin(self.base_url, href)
                })

            return {
                "title": title,
                "books": available_books,
                "source_url": self.current_url
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_book(self, content: str):
        """è§£æä¹¦ç±åˆ—è¡¨çš„å¼ºåŒ–ç‰ˆæœ¬"""
        try:
            html = BeautifulSoup(content, 'lxml')
            available_chapters = []

            # ä½¿ç”¨æ›´ç¨³å¥çš„é€‰æ‹©å™¨
            # title = html.find('title').get_text(strip=True) if html.title else "No Title"
            book_name = html.select('h2.text-sm.inline.ml-2.max-sm\\:hidden')
            # ä½¿ç”¨CSSé€‰æ‹©å™¨æé«˜å‡†ç¡®æ€§
            chapter_items = html.select('a.text-danger.hover\\:text-hover1[href]')

            # è·å–ç« èŠ‚åå’Œå¯¹åº”çš„url
            for chapter_link in chapter_items:
                if not (chapter_name := chapter_link.get_text(strip=True)):
                    continue

                if not (href := chapter_link.get('href')):
                    continue
                available_chapters.append({
                    "chapter_name": chapter_name,
                    "url": urljoin(self.base_url, href)
                })

            return {
                "book_name": book_name,
                "chapters": available_chapters,
                "source_url": self.current_url
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}", exc_info=True)
            return None

    # è§£æå•ä¸ªç« èŠ‚æ ‡é¢˜åŠå†…å®¹
    def _extract_chapter(self, content: str) -> Optional[Dict]:
        try:
            html = BeautifulSoup(content, 'lxml')
            # ä½¿ç”¨ select_one è·å–å•ä¸ªå…ƒç´ 
            chapter_title = html.select_one('h2.text-danger.text-center.text-lg.font-bold.mt-2')
            chapter_name = chapter_title.get_text(strip=True) if chapter_title else None
            # è·å–æ‰€æœ‰ c-en å…ƒç´ å¹¶æ‹¼æ¥å†…å®¹
            content_elements = html.select('div.c-en')
            chapter_content = "\n".join([
                elem.get_text(strip=True)
                for elem in content_elements
            ])
            return {
                "chapter_name": chapter_name,
                "content": chapter_content,
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def crawl(self, max_articles: int = 3) -> None:
        """æ‰§è¡Œçˆ¬å–æµç¨‹ï¼ˆå¼ºåŒ–é”™è¯¯å¤„ç†ç‰ˆæœ¬ï¼‰"""
        try:
            while max_articles > 0 and self.current_url:
                # å»é‡æ£€æŸ¥
                if self.current_url in self.visited_urls:
                    logger.warning(f"â© è·³è¿‡é‡å¤URL: {self.current_url}")
                    break

                # è·å–é¡µé¢å†…å®¹
                self.random_delay()
                content = self.fetcher.fetch_and_save(
                    url=self.current_url,
                    direction="yinyu",
                    save_origin=True
                )

                # å†…å®¹æœ‰æ•ˆæ€§éªŒè¯
                if not content:
                    logger.error(f"ğŸ›‘ è·å–å†…å®¹å¤±è´¥: {self.current_url}")
                    break

                # è§£æä¹¦ç±åˆ—è¡¨
                if not (booklists := self._extract_books(content)):
                    logger.error("ğŸ›‘ è§£æä¹¦ç±åˆ—è¡¨å¤±è´¥")
                    break

                # è½®è¯¢ä¹¦ç±åˆ—è¡¨
                for book in booklists.get("books", [])[:max_articles]:
                    logger.info(f"ğŸ“š æ­£åœ¨å¤„ç†: {book['name']}")
                    book_url = book["url"]
                    book_name = book["name"]
                    if book_url:
                        # è·å–é¡µé¢å†…å®¹
                        self.random_delay()
                        content = self.fetcher.fetch_and_save(
                            url=book_url,
                            direction="yinyu/{}".format(book_name),
                            save_origin=True
                        )
                        logger.info(book_url)
                        # è½®è¯¢æ¯æœ¬ä¹¦ç« èŠ‚åˆ—è¡¨
                        if chapter_lists := self._extract_book(content):
                            for chapter in chapter_lists.get("chapters", []):
                                chapter_url = chapter["url"]
                                logger.info(chapter_url)
                                # è·å–é¡µé¢å†…å®¹
                                self.random_delay()
                                content = self.fetcher.fetch_and_save(
                                    url=chapter_url,
                                    file_name="{}-{}.html".format(book_name, chapter["chapter_name"]),
                                    direction="yinyu/{}".format(book_name),
                                    save_origin=True
                                )
                                # ä¿å­˜ç« èŠ‚å†…å®¹
                                if chapter_data := self._extract_chapter(content):
                                    # ç»„åˆå…ƒæ•°æ®
                                    save_data = {
                                        "book_name": book_name,
                                        "chapter_name": chapter_data["chapter_name"],
                                        "chapter_url": chapter_url,
                                        "content": chapter_data["content"]
                                    }
                                    self._txt_saver([save_data])  # åŒ…è£¹æˆåˆ—è¡¨ä¿æŒæ¥å£ç»Ÿä¸€

                        else: logger.info("ğŸ›‘ è§£æç« èŠ‚åˆ—è¡¨å¤±è´¥")
                    max_articles -= 1
                    if max_articles <= 0:
                        break

        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            logger.error(f"çˆ¬å–æµç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            logger.info(f"ğŸ å‰©ä½™å¾…çˆ¬æ•°é‡: {max_articles}")
            logger.info("ğŸ‰ ä»»åŠ¡å®Œæˆ")

    def _txt_saver(self, data, base_dir="parsed"):
        """ä¿å­˜å™¨ï¼šparsed/{book_name}/{chapter_name}.txt"""
        for item in data:
            if not all(key in item for key in ["book_name", "chapter_name", "content"]):
                logger.warning("âš ï¸ æ•°æ®å­—æ®µç¼ºå¤±ï¼Œè·³è¿‡ä¿å­˜")
                continue
            try:
                # å‡†å¤‡è·¯å¾„ç»„ä»¶
                book_name = item["book_name"]
                raw_chapter_name = item["chapter_name"]

                # åˆ›å»ºå®‰å…¨æ–‡ä»¶å
                clean_chapter_name = re.sub(r'[\\/*?:"<>|]', '', raw_chapter_name)
                clean_chapter_name = clean_chapter_name.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦
                filename = f"{clean_chapter_name}.txt"

                # åˆ›å»ºç›®å½•ç»“æ„
                save_path = Path(base_dir) / book_name
                save_path.mkdir(parents=True, exist_ok=True)

                # å†™å…¥æ–‡ä»¶å†…å®¹
                with (save_path / filename).open('w', encoding='utf-8') as f:
                    f.write(f"Book: {book_name}\n")
                    f.write(f"Chapter: {raw_chapter_name}\n")
                    f.write(f"URL: {item.get('chapter_url', '')}\n\n")
                    f.write(item["content"])

                logger.info(f"âœ… æˆåŠŸä¿å­˜ï¼š{save_path / filename}")

            except Exception as e:
                logger.error(f"ğŸ›‘ ä¿å­˜å¤±è´¥: {str(e)}", exc_info=True)