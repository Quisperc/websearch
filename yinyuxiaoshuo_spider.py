# yinyu_Spider.py
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from typing import Optional, List, Dict
from pandas.core.generic import bool_t
from utils.BaseSpider import BaseSpider
from utils.TqdmLogHandler import logger

class yinyuSpider(BaseSpider):
    """è‹±è¯­å°è¯´ç« èŠ‚çˆ¬è™«"""

    def __init__(self, config=None):
        super().__init__("yinyu", config or {})
        self.base_url = "https://www.yingyuxiaoshuo.com/"
        self.visited_urls = set()
        self.timeout = self.config.get("timeout", 10)
        self.current_url = self.base_url  # è·Ÿè¸ªå½“å‰çˆ¬å–URL

    def _extract_books(self, content: str) -> Optional[Dict]:
        """è§£æä¹¦ç±åˆ—è¡¨çš„å¼ºåŒ–ç‰ˆæœ¬"""
        try:
            html = BeautifulSoup(content, 'lxml')
            available_books = []

            # ä½¿ç”¨æ›´ç¨³å¥çš„é€‰æ‹©å™¨
            title = html.find('title').get_text(strip=True) if html.title else "No Title"

            # ä½¿ç”¨CSSé€‰æ‹©å™¨æé«˜å‡†ç¡®æ€§
            # book_items = html.select('h2.inline.text-danger.hover\\:text-hover1.font-semibold a')
            # ç­›é€‰è‹±æ–‡å
            book_items = html.select('h2.inline.italic.text-xs.text-gray1.hover\\:text-hover1.max-sm\\:hidden.ml-2 a')

            for book_link in book_items:
                if not (name := book_link.get_text(strip=True)):
                    continue

                if not (href := book_link.get('href')):
                    continue

                available_books.append({
                    "name": name,
                    "url": urljoin(self.base_url, href)
                })

            return {
                "title": title,
                "books": available_books,
                "source_url": self.current_url
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_book(self, content: str):
        """è§£æä¹¦ç±åˆ—è¡¨çš„å¼ºåŒ–ç‰ˆæœ¬"""
        try:
            html = BeautifulSoup(content, 'lxml')
            available_chapters = []

            # ä½¿ç”¨æ›´ç¨³å¥çš„é€‰æ‹©å™¨
            # title = html.find('title').get_text(strip=True) if html.title else "No Title"
            book_name = html.select('h2.text-sm.inline.ml-2.max-sm\\:hidden')
            # ä½¿ç”¨CSSé€‰æ‹©å™¨æé«˜å‡†ç¡®æ€§
            chapter_items = html.select('a.text-danger.hover\\:text-hover1[href]')

            # è·å–ç« èŠ‚åå’Œå¯¹åº”çš„url
            for chapter_link in chapter_items:
                if not (chapter_name := chapter_link.get_text(strip=True)):
                    continue

                if not (href := chapter_link.get('href')):
                    continue
                available_chapters.append({
                    "chapter_name": chapter_name,
                    "url": urljoin(self.base_url, href)
                })

            return {
                "book_name": book_name,
                "chapters": available_chapters,
                "source_url": self.current_url
            }
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_chapter(self, content: str) -> Optional[Dict]:
        html = BeautifulSoup(content, 'lxml')

    def _get_next_page(self, booklists: Dict) -> Optional[str]:
        """åˆ†é¡µé€»è¾‘ç¤ºä¾‹ï¼ˆéœ€æ ¹æ®å®é™…ç½‘ç«™å®ç°ï¼‰"""
        # ç¤ºä¾‹ï¼šå‡è®¾é¡µé¢æœ‰åŒ…å« next page çš„é“¾æ¥
        # next_page = booklists.get('html').find('a', text='Next Page')
        # return urljoin(self.base_url, next_page['href']) if next_page else None
        return None  # æš‚æ—¶ç¦ç”¨åˆ†é¡µ

    def crawl(self, max_articles: int = 3) -> None:
        """æ‰§è¡Œçˆ¬å–æµç¨‹ï¼ˆå¼ºåŒ–é”™è¯¯å¤„ç†ç‰ˆæœ¬ï¼‰"""
        try:
            while max_articles > 0 and self.current_url:
                # å»é‡æ£€æŸ¥
                if self.current_url in self.visited_urls:
                    logger.warning(f"â© è·³è¿‡é‡å¤URL: {self.current_url}")
                    break

                # è·å–é¡µé¢å†…å®¹
                self.random_delay()
                content = self.fetcher.fetch_and_save(
                    url=self.current_url,
                    direction="yinyu",
                    save_origin=True
                )

                # å†…å®¹æœ‰æ•ˆæ€§éªŒè¯
                if not content:
                    logger.error(f"ğŸ›‘ è·å–å†…å®¹å¤±è´¥: {self.current_url}")
                    break

                # è§£æä¹¦ç±åˆ—è¡¨
                if not (booklists := self._extract_books(content)):
                    logger.error("ğŸ›‘ è§£æä¹¦ç±åˆ—è¡¨å¤±è´¥")
                    break

                # å¤„ç†ä¹¦ç±æ¡ç›®
                for book in booklists.get("books", [])[:max_articles]:
                    logger.info(f"ğŸ“š æ­£åœ¨å¤„ç†: {book['name']}")
                    book_url = book["url"]
                    book_name = book["name"]
                    if book_url:
                        # è·å–é¡µé¢å†…å®¹
                        self.random_delay()
                        content = self.fetcher.fetch_and_save(
                            url=book_url,
                            direction="yinyu/{}".format(book_name),
                            save_origin=True
                        )
                        logger.info(book_url)
                        if chapter_lists := self._extract_book(content):
                            for chapter in chapter_lists.get("chapters", []):
                                chapter_url = chapter["url"]
                                logger.info(chapter_url)
                                # è·å–é¡µé¢å†…å®¹
                                self.random_delay()
                                content = self.fetcher.fetch_and_save(
                                    url=chapter_url,
                                    file_name="{}-{}".format(book_name, chapter["chapter_name"]),
                                    direction="yinyu/{}".format(book_name),
                                    save_origin=True
                                )
                        else: logger.info("ğŸ›‘ è§£æç« èŠ‚åˆ—è¡¨å¤±è´¥")

                    max_articles -= 1
                    if max_articles <= 0:
                        break

                # è·å–ä¸‹ä¸€é¡µ
                if (next_url := self._get_next_page(booklists)) and next_url != self.current_url:
                    self.visited_urls.add(self.current_url)
                    self.current_url = next_url
                else:
                    break  # æ— æ›´å¤šé¡µé¢

        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            logger.error(f"çˆ¬å–æµç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
        finally:
            logger.info(f"ğŸ å‰©ä½™å¾…çˆ¬æ•°é‡: {max_articles}")
            logger.info("ğŸ‰ ä»»åŠ¡å®Œæˆ")
