# CNNSpider.py
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

from lxml import etree

from utils import Fetcher, Saver
import time
import random

class CNNSpider:
    def __init__(self):
        # CNNæ–‡ç« URLç¤ºä¾‹ï¼šhttps://edition.cnn.com/2023/08/15/tech/ai-robot-hand-write/index.html
        self.base_urls = [
            "https://edition.cnn.com/world",
            "https://edition.cnn.com/politics",
            "https://edition.cnn.com/business",
            "https://edition.cnn.com/tech"
        ]
        self.fetcher = Fetcher()
        self.delay_range = (1, 3)  # éšæœºå»¶è¿ŸèŒƒå›´
        self.thread_workers = 5  # æ–°å¢å¯é…ç½®å‚æ•°
        self.request_timeout = 10  # æ–°å¢è¶…æ—¶æ§åˆ¶

    def get_article_links(self, content):
        """ä»åˆ—è¡¨é¡µè·å–æ–‡ç« é“¾æ¥"""
        html = etree.HTML(content)
        return html.xpath('//a[contains(@href, "/202")]/@href')  # ç‰¹å¾é“¾æ¥

    def crawl_section(self, section_url):
        """çˆ¬å–å•ä¸ªæ¿å—"""
        print(f"â³ å¼€å§‹çˆ¬å–æ¿å—: {section_url}")
        content = self.fetcher.fetch_and_save(section_url,False)
        if not content:
            return []

        # æå–æ–‡ç« é“¾æ¥
        links = self.get_article_links(content)
        print(f"ğŸ”— å‘ç° {len(links)} ç¯‡æ–°é—»")

        # è¿‡æ»¤æ— æ•ˆé“¾æ¥å¹¶æ·»åŠ åŸŸå
        articles = []
        for link in set(links):
            if link.startswith('/202'):
                full_url = f"https://edition.cnn.com{link}"
                articles.append(full_url)
        return articles

    def crawl_article(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        time.sleep(random.uniform(*self.delay_range)) # éšæœºå»¶è¿Ÿ
        # ç»™fetch_and_saveæ·»åŠ è¶…æ—¶å‚æ•°
        # content = self.fetcher.fetch_and_save(url, False, timeout=self.request_timeout)
        content = self.fetcher.fetch_and_save(url, False)
        return Parser.parse_cnn(content, url) if content else None

    def crawl(self, max_articles=50):
        """ä¸»çˆ¬å–æ–¹æ³•"""
        all_articles = []

        # ç¬¬ä¸€æ­¥ï¼šçˆ¬å–å„æ¿å—è·å–æ–‡ç« é“¾æ¥
        for section in self.base_urls:
            all_articles.extend(self.crawl_section(section))
            if len(all_articles) >= max_articles:
                break
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œçˆ¬å–å…·ä½“æ–‡ç« 
        results = []
        articles_to_crawl = all_articles[:max_articles]

        # ä½¿ç”¨çº¿ç¨‹æ± ï¼ˆæ§åˆ¶åœ¨5ä¸ªå¹¶å‘ä»¥å†…ï¼‰
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                executor.submit(self.crawl_article, url): url
                for url in articles_to_crawl
            }

            for i, future in enumerate(as_completed(futures)):
                url = futures[future]
                try:
                    if article_data := future.result():
                        results.append(article_data)
                        print(f"âœ… å®Œæˆæ–‡ç«  ({len(results)}/{len(articles_to_crawl)})ï¼š{url}")
                except Exception as e:
                    print(f"â›” å¤„ç†å¤±è´¥: {url} - {str(e)}")
        # ä¿å­˜ç»“æœ
        if results:
            Saver.save_data(results)
            print(f"âœ… æˆåŠŸä¿å­˜ {len(results)} ç¯‡æ–°é—»")
        else:
            print("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆæ–°é—»")


class Parser:
    @staticmethod
    def parse_cnn(content,url=None):
        """è§£æCNNæ–‡ç« å†…å®¹"""
        html = etree.HTML(content)
        article_data = {
            "title": "No Title",
            "url": url,
            "author": "Unknown",
            "publish_time": "",
            "content": ""
        }

        # ä¼˜å…ˆä»ç»“æ„åŒ–æ•°æ®ä¸­æå–
        ld_json = None
        scripts = html.xpath('//script[@type="application/ld+json"]/text()')
        for script in scripts:
            try:
                data = json.loads(script.strip())
                if isinstance(data, list):
                    for item in data:
                        if item.get("@type") == "NewsArticle":
                            ld_json = item
                            break
                elif data.get("@type") == "NewsArticle":
                    ld_json = data
                if ld_json:
                    break
            except:
                continue

        # æ ‡é¢˜å¤„ç†
        if ld_json:
            article_data["title"] = ld_json.get("headline", "").strip()
            authors = []
            if "author" in ld_json:
                auth_list = ld_json["author"] if isinstance(ld_json["author"], list) else [ld_json["author"]]
                for auth in auth_list:
                    if isinstance(auth, dict):
                        authors.append(auth.get("name", "").strip())
            article_data["author"] = ", ".join(filter(None, authors)) or "Unknown"
            article_data["publish_time"] = ld_json.get("datePublished", "").strip()
            # article_data["content"] = ld_json.get("articleBody", "").replace("\n\n", "\n").strip()

        # é™çº§å¤„ç†ï¼šä»HTMLå…ƒç´ æå–
        if not article_data["title"]:
            title_elem = html.xpath('//h1/text()')
            article_data["title"] = title_elem[0].strip() if title_elem else "No Title"

        if article_data["author"] == "Unknown":
            author_elems = html.xpath('//div[contains(@class, "byline__name")]//text()')
            authors = [a.strip() for a in author_elems if a.strip()]
            article_data["author"] = ", ".join(authors) or "Unknown"

        if not article_data["publish_time"]:
            time_elem = html.xpath('//div[contains(@class, "timestamp")]/text()')
            article_data["publish_time"] = time_elem[0].strip() if time_elem else ""

        # if not article_data["content"]:
        #     content_paras = html.xpath('//div[contains(@class, "article__content")]//p[not(@class)]//text()')
        #     article_data["content"] = "\n".join(p.strip() for p in content_paras if p.strip())
        # å¼ºåˆ¶ä½¿ç”¨HTMLå…ƒç´ æå–ï¼ˆä¿è¯åˆ†æ®µï¼‰
        content_paras = []
        # é€šè¿‡æ›´ç²¾å‡†çš„XPathå®šä½æ®µè½
        para_nodes = html.xpath(
            '//div[contains(@class, "article__content")]//p[@class="paragraph inline-placeholder vossi-paragraph"]')
        for p in para_nodes:
            # æå–æ®µè½å†…æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬å­èŠ‚ç‚¹ï¼‰
            texts = p.xpath('.//text()')
            cleaned = ' '.join([t.strip() for t in texts if t.strip()])
            if cleaned:
                content_paras.append(cleaned)
        # ç”¨åŒæ¢è¡Œç¬¦è¿æ¥æ®µè½
        article_data["content"] = '\n\n'.join(content_paras)

        return article_data
